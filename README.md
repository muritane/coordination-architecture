# A Core Theory of Action for Bounded Agents

*(Exact-Authorization Revision — Executable Boundary Form, Clarified)*

---

## 1. The Problem

Bounded agents must act in environments that are:

* partially observable,
* noisy and non-stationary,
* costly or unsafe to explore,
* and sometimes adversarial.

Errors cannot always be explored, undone, or amortized. Many failures are irreversible **along the agent’s local trajectory**, even when predictable in hindsight. As a result, failures recur across biological, artificial, and institutional systems despite accumulated experience.

**The central question is therefore:**

> How can a bounded agent act reliably over time when mistakes cannot be safely explored, reversed, or delegated—and when coordination and scale require exact, repeatable execution?

---

## 2. The Irreducible Fact

**Action entails irreversible information loss along the agent’s local trajectory.**

Any pipeline from
*world → perception → representation → model → authorization → action*
maps many possible world states to fewer internal distinctions and ultimately to a single executed transition.

This mapping is:

* many-to-one,
* locally non-invertible for the acting agent,
* and irreversible at the time of execution.

Action does not merely select outcomes; it **collapses future option space** by committing the agent to one branch of reality while foreclosing others.

Exact execution does not remove this loss—it **stabilizes it**.

---

## 3. Exactness, Scale, and the Real Design Problem

Large-scale agents only function because some layers are **exactly specified and mechanically executable**:

* machine instructions resolve to specific registers and bit patterns,
* genetic replication operates on exact nucleotide sequences,
* laws, protocols, and procedures are written and publicly accessible.

Without exact templates, replication, coordination, and enforcement fail.

However:

> **Exact specification of symbols is not equivalent to complete specification of admissible state transitions.**

Exactness enables scale, but it does not guarantee safety, viability, or boundary completeness.

The true design problem is therefore not precision, but **authorization**.

---

## 4. Reframing the Design Problem

Given unavoidable local irreversibility, the primary design problem is not action selection or optimization per se, but:

> **Which distinctions, transitions, and commitments are explicitly authorized at the agent boundary—and under what conditions they may be revised.**

*Viability* refers strictly to maintaining the minimal preconditions for:

* continued action,
* learning and coordination,
* and redesign authority

**at the agent’s boundary and timescale.**

This framework does **not** determine which goals or values an agent should pursue.
Instead, it specifies **where and how value commitments must be made explicit**, enforced, and revisable under bounded resources and irreversible loss.

Intelligence is therefore reframed as a problem of:

* executable representation design,
* explicit boundary authorization,
* and preserved redesign authority,

with optimization operating strictly *inside* authorized regions.

---

## 5. Central Hypothesis (Constraint-as-Whitelist)

**Constraints function as explicit, exact whitelists over admissible actions, commitments, and state transitions.**

More precisely:

> A whitelist is an executable authorization schema that treats all non-specified transitions as inadmissible by default, while collapsing permitted states into equivalence classes for efficient control.

Whitelists do **not** require enumerating all atomic actions.
They operate at **chosen abstraction levels**, authorizing families of transitions parametrically under explicit scope boundaries.

Whitelist design is thus:

* proactive rather than reactive,
* authorization rather than prohibition,
* structural rather than behavioral.

Blacklisting (enumerating forbidden actions) does not scale under bounded knowledge.
**Whitelisting is the only scalable boundary strategy for agents whose actions are irreversible and whose coordination requires exact execution.**

---

## 6. Exact Authorization vs. Implicit Permission

Exact systems fail not because they are imprecise, but because:

* authorization is implicit rather than explicit,
* scope boundaries are assumed rather than claimed,
* interfaces are treated as neutral plumbing,
* or redesign authority is fragmented or undefined.

Adversarial success almost never comes from violating explicit rules.
It comes from **entering regions where authorization was never specified, monitored, or revisable.**

Any boundary region that cannot trigger redesign is already outside the agent’s sovereignty.

---

## 7. Safe Information Loss and Conditional Safety

Information destruction and action authorization are provisionally safe only when discarded distinctions or disallowed transitions:

* do not participate in causal pathways critical to viability,
* do not silently consume future agency, resources, or identity,
* remain invariant over the expected operating regime and horizon.

Safety is therefore:

* conditional on maintained authorization boundaries,
* probabilistic rather than absolute,
* and time-bounded.

No agent can certify safety *ex ante*.
It can only define **explicitly authorized regions**, enforce them mechanically, and monitor for violations that trigger redesign.

When redesign authority is lost or delayed, safety claims collapse.

---

## 8. Phase Structure of Whitelist-Controlled Agents

### Phase 1: Exploration (Whitelist Construction)

* Preserve degrees of freedom.
* Restrict action to explicitly safe exploration regions.
* Delay irreversible commitments.
* Discover causal structure.

Exploration occurs **inside provisional whitelists**, not in unconstrained space.

---

### Phase 2: Compression (Whitelist Fixation)

* Identify stable causal invariants.
* Encode exact, executable authorization rules.
* Collapse distinctions into admissible equivalence classes.

Compression defines what the agent is **allowed to do**, not merely what it predicts.

---

### Phase 3: Exploitation (Whitelist Execution)

* Act cheaply, repeatably, and at scale.
* Stay within authorized boundaries.
* Improve performance without expanding scope.

Efficiency increases while adaptability declines.

---

### Phase 4: Redesign (Whitelist Revision)

Triggered when:

* errors cluster,
* violations cannot be decorrelated,
* outcomes escape authorized bounds.

Redesign explicitly **reclaims boundary authority**, reintroducing distinctions and revising authorization.

Redesign authority is a first-class survival variable.

---

## 9. Models Are Also Whitelist-Constrained

Models are compressed representations operating **inside authorized abstractions**.

Therefore:

* models age,
* models mislead if treated as complete,
* model redesign follows the same whitelist logic as policy redesign.

There is no final model—only **phase-appropriate, executable authorization under bounded resources**.

---

## 10. Values, Consent, and Governance

Although this framework does not prescribe goals, it implies:

> Every authorization decision encodes values by determining which transitions are permitted, for how long, and under whose authority.

Legitimate governance therefore requires:

* public access to authorization rules,
* explicit scope boundaries,
* symmetric exposure to irreversible risk,
* preserved redesign authority.

Governance failures are boundary failures.

---

## 11. Core Synthesis (Exact Whitelist Form)

**Intelligent action is the phase-appropriate, exact authorization of a deliberately limited action space—chosen so that future action and redesign remain viable—until violations of assumed invariants force explicit revision under irreversible loss.**

This explains:

* why abstraction is unavoidable,
* why exactness enables scale but accelerates failure when misapplied,
* why efficiency creates brittleness,
* why failures cluster,
* and why authority over redesign determines survival.

---

## Scope Clarification

This framework applies to biological, artificial, institutional, and hybrid agents wherever action requires **exact execution under irreversible abstraction and bounded resources**.

It is a boundary logic, not a learning algorithm, optimization method, or moral theory.

---

## What Is Intentionally Deferred to Appendices

The following are valuable but non-core and should live outside the core paper:

* detailed causal class taxonomies,
* adversarial and deceptive stability regimes,
* formal diagnostic metrics,
* compression registers and governance mechanisms,
* institutional and political failure modes,
* explicit critiques of over-formal optimization.

Including these would risk over-closure, premature generality, and reduced accessibility.

---

## Why This Core Holds

This framework relies only on unavoidable facts:

* resources are bounded,
* observations are partial,
* action is locally irreversible,
* scale requires exact templates,
* boundaries determine agency,
* redesign authority preserves viability.

Everything else is downstream.

---

## License

This work is licensed under the **Creative Commons Attribution 4.0 International License (CC BY 4.0)**.

You are free to share and adapt this material for any purpose, including commercial use, **provided appropriate attribution is given**.

License text:
[https://creativecommons.org/licenses/by/4.0/](https://creativecommons.org/licenses/by/4.0/)
