# Metrics Under Constraint

## Field Notes on When Evaluation Becomes Comparable, Executable, and Fair at Scale

---

## What This Document Is

This document specifies **metrics-for-evaluation** as *system-level artifacts* that arise only under **execution, coordination, and resource constraints**.

It treats metrics not as neutral measurements or moral judgments, but as **operational instruments** that must:

- be externally addressable,
- admit substitution across heterogeneous systems,
- operate under bounded resources,
- survive scale,
- and incur failure cost when misapplied.

The framework is architectural and execution-oriented.  
It is isomorphic to principles from:

- systems engineering,
- performance modeling,
- information theory,
- control theory,
- and distributed evaluation under partial observability.

Metrics are treated as **executable evaluative artifacts**, not as reflections of intrinsic worth, intelligence, or value.

---

## What This Document Is Not

This document does **not** attempt to:

- define value, merit, or worth,
- rank humans, systems, or institutions,
- prescribe moral or political outcomes,
- reduce qualitative differences to numbers,
- argue for a single universal metric,
- deny the legitimacy of subjective judgment.

It addresses one boundary only:

> When an evaluation claim becomes **comparable-for-coordination** rather than an internal or local judgment.

---

## Core Claim

A metric becomes *real-for-evaluation* only when it is **minimally sufficient under the constraints of execution and comparison**.

Metrics that ignore:
- unused degrees of freedom,
- coordination overhead,
- failure surfaces,
- or constraint regimes,

are **not incorrect** — they are **non-executable for fair comparison**.

This is not a semantic failure.  
It is an **architectural insufficiency**.

---

## Metrics as Executable Artifacts

A metric is an executable artifact if it:

- can be applied by substituted evaluators,
- yields determinate outcomes under a protocol,
- supports repeated execution,
- and incurs meaningful failure cost when misapplied.

Metrics that survive only through:
- reinterpretation,
- contextual excuses,
- or post hoc adjustment,

have failed as shared evaluative artifacts.

They may still function as **local heuristics**.

---

## Internal Judgment vs Shared Evaluation

### Internal Judgment (Non-Executable Evaluation)

Internal judgment includes:

- personal intuition,
- informal comparison,
- local standards,
- tacit expertise,
- context-rich appraisal.

Internal judgment:
- is high-dimensional,
- unconstrained by substitution,
- tolerant of inconsistency,
- resilient through reinterpretation,
- and incurs no systemic failure cost.

It is real for the evaluator.  
It is **non-addressable by systems**.

---

### Shared Metrics (Executable Evaluation)

Shared metrics-for-evaluation:

- externalize judgment,
- discretize criteria,
- impose admissible states,
- terminate interpretation,
- and enable coordination.

A shared metric must be executable **without access to the originator’s internal standards or intent**.

> Evaluation that cannot fail cannot coordinate.  
> Evaluation that survives only by reinterpretation has failed *as a metric*.

---

## Minimal Sufficiency in Metrics

A metric is **minimally sufficient** if it captures **exactly the degrees of freedom required** to support the comparison it claims to make — and no more.

Minimal sufficiency is not minimalism.  
It is **state-space control**.

---

### Insufficient Metrics

A metric is insufficient when it:

- collapses heterogeneous systems into a single axis,
- ignores unused or latent degrees of freedom,
- omits coordination or control costs,
- assumes identical constraint regimes,
- or treats generality as inefficiency.

Such metrics may be simple, but they are **non-executable at scale**.

---

### Over-Specified Metrics

A metric is over-specified when it:

- requires unbounded context,
- depends on private interpretation,
- cannot be measured reliably,
- or introduces DOFs that do not affect outcomes.

Over-specification increases coordination cost and reduces portability.

---

## Preconditions for Executable Metrics

A metric becomes shared-for-evaluation **only if all of the following hold**.

These are not moral requirements.  
They are **execution constraints**.

---

### 1. Bounded Resources

Evaluation consumes:
- time,
- attention,
- energy,
- compute,
- institutional capacity.

Metrics requiring infinite context or perfect knowledge are non-executable.

---

### 2. Repeated Application (Scale)

Metrics must support:
- repeated use,
- aggregation,
- comparison across instances.

One-off judgments do not expose metric insufficiency.  
Scale does.

---

### 3. Non-Zero Coordination Cost

Metrics impose:
- data collection,
- interpretation,
- enforcement,
- dispute resolution.

These costs must be accounted for, not externalized.

---

### 4. Partial Observability

Evaluators do not share full access to internal state.

Metrics must operate under:
- asymmetric information,
- delayed signals,
- incomplete context.

Metrics assuming full observability are non-executable.

---

### 5. Failure Cost

Misapplication of a metric must be able to:
- cause error,
- misallocate resources,
- trigger correction,
- or produce visible harm.

Metrics that cannot fail meaningfully do not constrain behavior.

---

### 6. Substitutability

Different evaluators must be able to apply the metric and reach compatible outcomes.

If evaluation depends on unique insight or intent, the metric is internal.

---

## Degrees of Freedom and the Generality Tax

Systems differ in:
- available DOFs,
- exercised DOFs,
- retained latent flexibility.

General-purpose systems:
- pay ongoing cost to preserve unused DOFs,
- retain adaptability as a reserve.

Specialized systems:
- collapse state-space,
- maximize throughput along narrow axes,
- discard flexibility.

Metrics that ignore this distinction impose a **generality tax** by construction.

---

## Fair Comparison Under Constraint

Fair evaluation does not require identical metrics.  
It requires **constraint-aligned metrics**.

Comparable systems must share:
- constraint regime,
- horizon,
- failure surface,
- and resource bounds.

Valid comparative dimensions include:

- effective throughput over time,
- energy-normalized output,
- DOF utilization efficiency,
- lifetime-adjusted contribution,
- adaptability reserve,
- recovery cost under failure.

No single scalar metric is sufficient across heterogeneous systems.

---

## Metric Failure Modes

Common failure modes include:

- collapsing generalists into specialist metrics,
- treating latent flexibility as waste,
- ignoring coordination overhead,
- deferring failure via reinterpretation,
- externalizing cost to unmeasured domains.

These failures produce **false comparability**, not incorrect numbers.

---

## Metrics as Interfaces

Metrics are interfaces between:
- observation and action,
- judgment and allocation,
- comparison and coordination.

Like all interfaces, metrics:
- cut continuous reality,
- discretize state,
- prevent illegal comparisons,
- and terminate debate.

Lossiness is required.

Metrics that refuse to cut do not scale.

---

## Protocol-Relative Evaluation

Metrics are **protocol-relative**.

A metric is valid only within:
- a defined task class,
- a constraint regime,
- an execution horizon.

Outside that protocol, evaluation is not wrong — it is **undefined-for-comparison**.

---

## Implications (Diagnostic, Not Normative)

- Most public metrics are not minimally sufficient.
- Metric disputes often reflect hidden DOF mismatches.
- Generalists are systematically mis-evaluated by specialist metrics.
- “Objectivity” often masks unacknowledged constraint assumptions.
- Better metrics reduce conflict by making trade-offs explicit.

---

## Metrics Under Constraint

The distinction is categorical:

- **Internal judgment** is unconstrained and local.
- **Shared metrics-for-evaluation** are constrained and executable.

Gradients exist **within** metrics, never at their boundary.

A metric that survives constraint becomes:
- inspectable,
- portable,
- enforceable,
- failure-bearing.

A metric that does not remains internal —
not by accident, but by architectural necessity.

---

## Closing Note

Metrics do not become fair by being universal.

They become fair by being **minimally sufficient under shared constraints**.

Everything else is preference,
intuition,
or power —
not evaluation.

