# Metrics Under Constraint  
## When Evaluation Becomes Comparable, Executable, and Load-Bearing at Scale

---

## Status and Intent

This document provides a **structural account of metrics-for-evaluation**.

Metrics are treated neither as neutral measurements nor as moral judgments, but as **executable evaluative artifacts** that arise only under **coordination, execution, and resource constraints**.

The framework is architectural and operational.  
It is isomorphic to principles from:

- systems engineering,
- control theory,
- distributed evaluation,
- performance modeling under partial observability.

Metrics are analyzed solely with respect to their ability to **support coordination, comparison, and allocation under constraint**.

---

## What This Document Is Not

This document does **not**:

- define value, merit, or worth,
- rank people, systems, or institutions,
- prescribe political or moral outcomes,
- deny the legitimacy of subjective judgment,
- argue for universal metrics.

It addresses one boundary only:

> When an evaluative claim becomes **shared-for-coordination** rather than internal, local, or discretionary.

Everything outside that boundary is out of scope.

---

## Core Claim

A metric becomes **real-for-evaluation** only when it is **minimally sufficient under execution and comparison constraints**.

Metrics that ignore:

- constraint heterogeneity,
- unused degrees of freedom,
- coordination overhead,
- failure surfaces,
- or horizon mismatch,

are not incorrect.  
They are **non-executable for fair comparison**.

This is not a semantic flaw.  
It is an **architectural insufficiency**.

---

## Metrics as Executable Artifacts

A metric is an executable artifact if it:

- can be applied by substituted evaluators,
- yields determinate outcomes under a protocol,
- supports repeated execution at scale,
- incurs visible failure cost when misapplied.

Metrics that survive only through:

- reinterpretation,
- contextual excuse,
- post hoc adjustment,

have failed as **shared evaluative artifacts**.

They may still function as **internal heuristics**.

---

## Internal Judgment vs Shared Evaluation

### Internal Judgment (Non-Executable Evaluation)

Internal judgment includes:

- intuition,
- tacit expertise,
- informal comparison,
- context-rich appraisal,
- local standards.

Internal judgment is:

- high-dimensional,
- unconstrained by substitution,
- tolerant of inconsistency,
- resilient through reinterpretation,
- free of systemic failure cost.

It is real for the evaluator.  
It is **non-addressable by systems**.

---

### Shared Metrics (Executable Evaluation)

Shared metrics-for-evaluation:

- externalize judgment,
- discretize criteria,
- define admissible states,
- terminate interpretation,
- enable coordination.

A shared metric must be executable **without access to the originator’s internal intent, taste, or standards**.

> Evaluation that cannot fail cannot coordinate.  
> Evaluation preserved only by reinterpretation has failed as a metric.

---

## Minimal Sufficiency

A metric is **minimally sufficient** if it captures **exactly the degrees of freedom required** to support the comparison it claims to make — and no more.

Minimal sufficiency is not minimalism.  
It is **state-space control**.

---

### Insufficient Metrics

A metric is insufficient when it:

- collapses heterogeneous systems into a single axis,
- ignores latent or unused degrees of freedom,
- omits coordination or control cost,
- assumes identical constraint regimes,
- treats preserved flexibility as inefficiency.

Such metrics may be simple.  
They are **non-executable at scale**.

---

### Over-Specified Metrics

A metric is over-specified when it:

- requires unbounded context,
- depends on private interpretation,
- cannot be measured reliably,
- introduces DOFs irrelevant to outcomes.

Over-specification increases coordination cost and reduces portability.

---

## Preconditions for Executable Metrics

A metric becomes shared-for-evaluation **only if all of the following constraints hold**.

These are execution constraints, not moral criteria.

---

### 1. Bounded Resources

Evaluation consumes:

- time,
- attention,
- energy,
- compute,
- institutional capacity.

Metrics requiring infinite context or perfect knowledge are non-executable.

---

### 2. Repeated Application

Metrics must support:

- repetition,
- aggregation,
- comparison across instances.

One-off judgments do not surface metric failure.  
Scale does.

---

### 3. Non-Zero Coordination Cost

Metrics impose:

- data collection,
- interpretation,
- enforcement,
- dispute resolution.

These costs must be priced, not externalized.

---

### 4. Partial Observability

Evaluators operate under:

- asymmetric information,
- delayed signals,
- incomplete state access.

Metrics assuming full observability are non-executable.

---

### 5. Failure Cost

Metric misapplication must be able to:

- misallocate resources,
- trigger correction,
- produce visible harm,
- or force redesign.

Metrics that cannot fail do not constrain behavior.

---

### 6. Substitutability

Different evaluators must be able to apply the metric and reach compatible outcomes.

If evaluation depends on unique insight or taste, the metric is internal.

---

## Degrees of Freedom and the Generality Tax

Systems differ in:

- available DOFs,
- exercised DOFs,
- retained latent flexibility.

General-purpose systems:

- preserve unused DOFs at ongoing cost,
- retain adaptability as reserve capacity.

Specialized systems:

- collapse state space,
- maximize throughput along narrow axes,
- discard flexibility.

Metrics that ignore this distinction impose a **generality tax by construction**.

---

## Fair Comparison Under Constraint

Fair evaluation does not require identical metrics.  
It requires **constraint-aligned metrics**.

Comparable systems must share:

- constraint regime,
- execution horizon,
- failure surface,
- resource bounds.

Valid comparative dimensions include:

- throughput normalized by time and energy,
- lifetime-adjusted output,
- adaptability reserve,
- recovery cost under failure,
- coordination overhead.

No single scalar metric suffices across heterogeneous systems.

---

## Metric Failure Modes

Recurring failure patterns include:

- collapsing generalists into specialist metrics,
- treating latent flexibility as waste,
- ignoring coordination cost,
- deferring failure via reinterpretation,
- externalizing cost to unmeasured domains.

These failures produce **false comparability**, not wrong numbers.

---

## Metrics as Interfaces

Metrics are interfaces between:

- observation and action,
- judgment and allocation,
- comparison and coordination.

Like all interfaces, metrics:

- cut continuous reality,
- discretize state,
- exclude illegal comparisons,
- terminate debate.

Lossiness is required.

Metrics that refuse to cut do not scale.

---

## Protocol-Relative Evaluation

Metrics are protocol-relative.

A metric is valid only within:

- a defined task class,
- a constraint regime,
- a declared horizon.

Outside that protocol, evaluation is not wrong —  
it is **undefined-for-comparison**.

---

## Metrics Under Constraint

The distinction is categorical:

- Internal judgment is unconstrained and local.
- Shared metrics-for-evaluation are constrained and executable.

Gradients exist **within** metrics, never at their boundary.

A metric that survives constraint becomes:

- inspectable,
- portable,
- enforceable,
- failure-bearing.

A metric that does not remains internal —
not by accident, but by architectural necessity.

---

## Closing

Metrics do not become fair by being universal.

They become fair by being **minimally sufficient under shared constraints**.

Everything else is preference,
intuition,
or power —
not evaluation.
